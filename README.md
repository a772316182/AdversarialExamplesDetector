# adversarial examples detector

**adversarial examples detector** is a application to test your unnoticeability of your adversarial examples. 

if your paper want to involves **experiments related to human perception of the unnoticeability of your adversarial examples**, this app will help you.

you may ask for volunteers to help you **manually** determine whether some images are adversarial examples.



## Usage

for start, you need to input the locations of the adversarial examples, each path should related to a attack method.

you also need to input the location of the clean examples.

<img src="https://github.com/a772316182/adversarial_examples_detector/raw/master/image-20221110202929095.png" alt="image-20221110202929095" style="zoom: 50%;" />

then, you need to checkout the adversarial examples.

 

<img src="https://github.com/a772316182/adversarial_examples_detector/raw/master/image-20221110202009167.png" alt="image-20221110202009167" style="zoom: 50%;" />



finally, you can see the answers.

<img src="https://github.com/a772316182/adversarial_examples_detector/raw/master/image-20221110202633682.png" alt="image-20221110202633682" style="zoom:80%;" />



## Download

This is a desktop application, powered by electron, and we have build **a example version** for Windows, you can download and unzip this file, then run **electron.exe** to start.

[portable] https://drive.google.com/file/d/13RySxNkzY4xek6o3J1ao8G1wj2ewtLYF/view?usp=share_link
